---
title: "Example: Hidden Markov Model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Example: Hidden Markov Model}
  %\VignetteEngine{knitr::rmarkdown}
bibliography: ref.bib
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.dim = c(8, 6), 
  out.width = "75%",
  eval = FALSE
)
#library("ino")
devtools::load_all() # remove later
set.seed(1)
hmm_ino <- ino::hmm_ino
```

This vignette^[The vignette was build using R `r paste(R.Version()[c("major","minor")], collapse = ".")` with the {ino} `r utils::packageVersion("ino")` package.] describes the workflow of the {ino} package for the likelihood optimization of an hidden Markov model (HMM). For more technical details about HMMs see, e.g., @zucchini2016.

## Data

The example data set considered throughout this vignette covers a time series of share returns from the stock 'Deutsche Bank'. The data set is freely accessible via Yahoo Finance. 

```{r message = FALSE, warning = FALSE, eval = TRUE}
library("dplyr")
# data <- fHMM::download_data(symbol = "DBK.DE", from = "2000-01-01", file = NULL)
# remove later
data <- read.csv("D:/sciebo/lasso_select_N/DB.csv") %>% slice(788:nrow(.))
db_data <- data %>%
  as_tibble() %>%
  summarize(date = as.Date(Date, format = "%Y-%m-%d"),
            obs = c(NA, diff(log(Close), lag=1) * 100)) %>%
  filter(!is.na(obs)) %>%
  print()
```

```{r, eval = TRUE}
library("ggplot2")
ggplot(db_data, aes(x = date, y = obs)) +
  geom_point() +
  geom_line() +
  ylab("log-returns [%]")
```

As the share returns are continuous and can take both negative and positive values, we consider an HMM with Gaussian state-dependent distributions. The (log-)likelihood of a Gaussian-HMM is implemented in the function `f_ll_hmm()`.

## Model fitting

We consider a 2-state (`N = 2`) Gaussian-HMM with six parameters (`npar = 6`) to be estimated:

- two for the transition probability matrix (t.p.m.)
- two for the means of the state-dependent distribution
- two for the standard deviations of the state-dependent distribution

The likelihood for a Gaussian-HMM is given by the function `f_ll_hmm()` provided by the {ino} package. 
The argument `neg =  TRUE` indicates that we minimize the negative log-likelihood, and `set_optimizer(optimizer_nlm())` selects the optimizer `nlm()`.

```{r, eval = FALSE}
hmm_ino <- Nop$new(
  f = f_ll_hmm,
  npar = 6,
  data = db_data,
  N = 2,
  neg = TRUE
)$set_optimizer(optimizer_nlm())
```

### Random initialization

We first use randomly chosen starting values. As the first two starting values belong to the t.p.m., we draw starting values from a $\mathcal{U}(-2,-1)$ distribution --- as we use the multinomial logit link to ensure that the probabilities are between 0 and 1, a value of -1.5 correspond to probabilities of staying in state 1 and 2 of about 0.81. For the two means, we draw two random numbers from the standard normal distribution, as the time series above indicates that the most of the returns are fairly close to zero. The starting values for the standard deviations are drawn from a $\mathcal{U}(0.5,2)$ distribution (note that we exponentiate the standard deviations in the likelihood as they are constrained to be positive, and hence we log transform the starting values).

The function `optimize` with argument `initial = sampler` then performs `runs = 50` optimization runs with starting values drawn from the selected distributions. Setting `save_results` to `TRUE` stores the results of the optimisation runs in `hmm_ino` so that we can access and compare them afterwards. For such comparisons with other optimization strategies (which will be carried out below), it's useful to set a label.

```{r, eval = FALSE}
sampler <- function() c(stats::runif(2, -2, -1),
                        stats::rnorm(2),
                        log(stats::runif(2, 0.5, 2)))
hmm_ino$optimize(initial = sampler, runs = 50, label = "random",
                 save_results = TRUE)
```

### Fixed initialization

For selecting fixed starting values, we consider three sets of starting values that fall in the ranges considered above:

```{r, eval = FALSE}
starting_values <- list(c(-2, -2, 0, 0, log(2), log(3)), 
                        c(-1.5, -1.5, -0.1, 0.1, log(1), log(2)),
                        c(-1, -1, -0.2, 2, log(2), log(2)))
for(val in starting_values)
  hmm_ino$optimize(initial = val, save_results = TRUE, label = "fixed")
```

### Subset initialization

To illustrate the subset initialization strategy, we consider the first 50\% of the observation using `reduce()`. The starting values for this subset are chosen randomly from the same distributions as considered above. We again use the function `optimize()` to fit the HMM, but now to a subset containing the first 50\% of the observations. With `continue()`, we then use the estimates obtained from the subsets as initial values to fit the model to the entire data set.

```{r, eval = FALSE}
hmm_ino$reduce("data", how = "first", prop = 0.5)
hmm_ino$optimize(initial = sampler, runs = 50, label = "subset_50")
hmm_ino$continue()
hmm_ino$reset_argument("data")
```

As a final step in the previous code snippet, we use `reset_argument()` to obtain our actual data set consisting of *all* observations. If we were to skip this step, all future optimization runs would be made on the subset.


As the time series of share returns consists of `r nrow(db_data)` observations, we can even try to use fewer observations to obtain initial starting values. In the next step, we consider only the first 5\% of the observations.

```{r, eval = FALSE}
hmm_ino$reduce("data", how = "first", prop = 0.05)
hmm_ino$optimize(initial = sampler, runs = 50, label = "subset_5")
hmm_ino$continue()
hmm_ino$reset_argument("data")
```

## Evaluating the optimization runs

### Local optima

Selecting the starting values for HMMs is a well-known issue, as poor starting values may likely result in local optima. Other R packages designed to fit HMMs discuss this topic in more detail (see, e.g., https://cran.r-project.org/package=moveHMM). We thus first evaluate the optimizations by comparing the likelihood values at convergence, which can be displayed using `optima()`:

```{r eval=TRUE}
hmm_ino$optima()
```

The frequency table indicates that `r hmm_ino$optima() %>% slice(1) %>% pull(frequency)` out of `r sum(hmm_ino$optima()$frequency)` runs converged to the same likelihood value, which appears to be the global optimum (note these are the negative log-likelihood values).  

Using `summary()`, we can investigate the computation time ("seconds"), the resulting optimum ("value") and the corresponding initialization strategy ("label") for all runs: 

```{r eval=TRUE}
hmm_ino$summary(c("value", "seconds", "label")) %>% 
  head(n = 10)
```


The final parameter estimates, i.e., the parameters associated with the global optimum, are stored as `best_parameter`:

```{r eval=TRUE}
hmm_ino$best_parameter
```

and the corresponding likelihood value can be obtained as

```{r eval=TRUE}
hmm_ino$best_value
```


### Optimization time

We use the `plot()` function to investigate the computation time. Intuitively, optimization runs with fixed starting values should be faster than with random starting values, since we have carefully chosen the fixed starting values by investigating the data. The boxplots confirm this intuition: 

```{r eval=TRUE}
plot(hmm_ino, by = "label", nrow = 1)
```

Using the output provided by `summary()` and some data manipulation functions provided by the package {dplyr}, we can also compute the average time per strategy:

```{r warning = FALSE, message = FALSE, eval=TRUE}
summary(hmm_ino, c("label", "seconds")) %>% 
  group_by(label) %>% 
  summarise(avg_time = mean(seconds))
```

This comparison can however be considered somehow 'unfair', as not all optimization runs lead to the global optimum --- optimization runs that lead to local optima may then have less iterations and hence lower computation time. We can first check which runs lead to the apparent global optimum of `r hmm_ino$best_value`. For that, we will again use functions from {dplyr}.

```{r eval=TRUE}
summary(hmm_ino, c("label", "value", "seconds")) %>% 
  mutate(global_optimum = ifelse(value < 13152, 1, 0)) %>% 
  group_by(label) %>% 
  summarise(proportion_global_optimum = mean(global_optimum))
```

While for the fixed starting values all runs converge to the global optimum, the random initialization and subset initialization strategies do sometimes get stuck in local maxima.

Let's again compare the average computation time, but only for those runs that lead to the global optimum:

```{r eval=TRUE}
summary(hmm_ino, c("label", "value", "seconds")) %>% 
  filter(value < 13152) %>% 
  group_by(label) %>% 
  summarise(mean_time = mean(seconds))
```

For those runs that converged to the global optimum, we can again compare the computation time via boxplots:

```{r eval=TRUE}
summary(hmm_ino, c("label", "value", "seconds")) %>% 
  filter(value < 13152) %>% 
  ggplot(aes(x = "", y = seconds)) +
    scale_y_continuous() +
    geom_boxplot() +
    facet_wrap("label", labeller = "label_both", nrow = 1) +
    theme(
      axis.title.x = element_blank(),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank()
    ) +
    ylab("seconds")
```

While computation time is lowest for the fixed initialization strategy, subset initialization appears to be a very promising approach. In particular, when considering the first 5% of the data in a first step, the computation time is most likely to be lower compared to the random initialization approach.

## References
