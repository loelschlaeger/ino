---
title: "Introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.dim = c(8, 6), 
  out.width = "75%"
)
library("ino")
# devtools::load_all() # remove later
options("ino_verbose" = FALSE)
options("ino_ncores" = 1)
options("ino_digits" = 2)
set.seed(1)
```

## Motivation behind {ino}

Optimization is of great relevance in many fields, including finance (portfolio optimization), engineering (minimizing air resistance), and statistics (likelihood maximization for model fitting). Often, the optimization problem at hand cannot be solved analytically, for example when explicit formulas for gradient or Hessian are unknown. In these cases, numerical optimization algorithms are helpful. They iteratively explore the parameter space, guaranteeing to improve the function value over each iteration, and eventually converge to a point where no more improvements can be made [@Bonnans:2006]. In R, several functions are available that can be applied to numerically solve optimization problems, including (quasi) Newton (`stats::nlm()`, `stats::nlminb()`, `stats::optim()`), direct search (`pracma::nelder_mead()`), and conjugate gradient methods (`Rcgmin::Rcgmin()`). The [CRAN Task View: Optimization and Mathematical Programming](https://CRAN.R-project.org/view=Optimization) provides a comprehensive list of packages for solving optimization problems. 

One thing that all of these numerical optimizers have in common is that initial parameter values must be specified, i.e., the point from where the optimization is started. Optimization theory [@Nocedal:2006] states that the choice of an initial point has a large influence on the optimization result, in particular convergence time and rate. In general, starting in areas of function saturation increases computation time, starting in areas of non-concavity leads to convergence problems or convergence to local rather than global optima. Consequently, numerical optimization can be facilitated by 

1. analyzing the initialization effect for the optimization problem at hand and

2. putting effort on identifying good starting values.

However, it is generally unclear what good initial values are and how they might affect the optimization. Therefore, the purpose of the {ino} R package (*ino* is an acronym for *i*nitialization of *n*umerical *o*ptimization) is to provide a comprehensive toolbox for 

1. evaluating the effect of the initial values on the optimization,

2. comparing different initialization strategies,

3. and comparing different optimizer.

## Functionality of {ino}

The {ino} package provides the R6 class `Nop` (an acronym for *n*umerical *o*ptimization *p*roblem).

- The starting point for working with {ino} is to define a `Nop` object via `Nop$new()`.

- Use the method `$set_optimizer()` to define one or more numerical optimizer.

- Then, `$evaluate()` evaluates and `$optimize()` optimizes the target function.

- For evaluating the results, `$optima()` prints an overview of all optima, and the `plot()` and `summary()` methods summarize the optimization runs.

- The method `$standardize()` can be used to standardize the optimization problem.

- The method `$reduce()` can be used to simplify the optimization problem.

## Working with {ino}

We demonstrate the basic {ino} workflow in the context of likelihood maximization, where we fit a two-class Gaussian mixture model to Geyser eruption times in the popular `faithful` data set that is provided via base R.

> **Remark:** Optimization in this example is very fast. This is because the data set is relatively small and we consider a model with only two classes. Therefore, it might not seem relevant to be concerned about initialization here. However, the problem scales: optimization time will rise with more data and more parameters, in which case initialization becomes a greater issue, see for example @Shireman:2017. Additionally, we will see that this simple optimization problem suffers heavily from local optima, depending on the choice of initial values.

### The optimization problem

The `faithful` data set provided by the {datasets} package contains information about eruption times of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The following histogram hints at two clusters with short and long eruption times, respectively:

```{r, include = FALSE, warning = FALSE}
library("ggplot2")
data("faithful")
```

```{r}
str(faithful)
ggplot(faithful, aes(x = eruptions, y = after_stat(density))) + 
  geom_histogram(bins = 30) + 
  xlab("Eruption time (min)") 
```

For both clusters, we assume a normal distribution here, such that we consider a mixture of two Gaussian densities for modeling the overall eruption times. The log-likelihood function is given by

$$
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \log\Big( \pi f_{\mu_1, \sigma_1^2}(x_i) + (1-\pi)f_{\mu_2,\sigma_2^2} (x_i) \Big),
$$

where $f_{\mu_1, \sigma_1^2}$ and $f_{\mu_2, \sigma_2^2}$ denote the normal density for the first and second cluster, respectively, and $\pi$ is the mixing proportion. The vector of parameters to be estimated is thus $\boldsymbol{\theta} = (\mu_1, \mu_2, \sigma_1, \sigma_2, \pi)$. As there exists no closed-form solution for the maximum likelihood estimator, we need numerical optimization for finding the function optimum. 

The following function calculates the log-likelihood value of the normal mixture, given the parameter vector `theta` and column `column` of the `data.frame` `data`. Note that we restrict the standard deviations `sigma` to be positive (via the exponential transformation) and `pi` to be between 0 and 1 (via the logit transformation), and that the function returns the negative log-likelihood value by default.

```{r}
normal_mixture_llk <- function(theta, data, column, neg = TRUE){
  stopifnot(length(theta) == 5)
  mu <- theta[1:2]
  sigma <- exp(theta[3:4])
  pi <- plogis(theta[5])
  x <- data[[column]]
  llk <- sum(log(pi * dnorm(x, mu[1], sigma[1]) + (1 - pi) * dnorm(x, mu[2], sigma[2])))
  ifelse(neg, -llk, llk)
}
```

### Specification in {ino}

The optimization problem is specified as a `Nop` R6 object called `geyser` below, where

- `f` constitutes the function to be optimized (i.e., `normal_mixture_llk`), 
- `npar` specifies the length of the parameter vector over which `f` is optimized (five in this case),
- `data` gives the `data.frame` `faithful` of observations as required by our likelihood function,
- and `column = "eruptions"` is an additional function argument required by `f`. 

```{r}
geyser <- Nop$new(
  f = normal_mixture_llk, 
  npar = 5, 
  data = faithful, 
  column = "eruptions"
)
```

The `print()` method provides an overview of the specification.

```{r}
print(geyser)
```

The next specification step concerns the numerical optimizer, which we can specify via the `$set_optimizer()` method. 

> **Remark:** Numerical optimizer must be specified through the unified framework provided by the [{optimizeR} package](https://CRAN.R-project.org/package=optimizeR). This is necessary, because there is no a priori consistency across optimization functions in R with regard to their function inputs and outputs. This would make it impossible to allow for arbitrary optimizer and to compare their results, see [the {optimizeR} README file for details](https://cran.r-project.org/package=optimizeR/readme/README.html).

It is possible to define any numerical optimizer through the {optimizeR} framework. Here, we select two of the most popular R optimizer, `stats::nlm()` and `stats::optim()`:

```{r}
geyser$
  set_optimizer(optimizer_nlm(), label = "nlm")$
  set_optimizer(optimizer_optim(), label = "optim")
```

Finally, we recommend to test the configuration with the `$test()` method:

```{r, R.options = list("ino_verbose" = TRUE)}
geyser$test()
```

### Function evaluation

Once the `Nop` object is specified, evaluating the `normal_mixture_llk` at some value for the parameter vector `theta` is simple with the `$evaluate()` method, for example:

```{r}
geyser$evaluate(at = 1:5)
```

### Function optimization

Optimization of `normal_mixture_llk` is possible with the `$optimize()` method, for example:

```{r}
geyser$optimize(initial = "random", which_optimizer = "nlm", save_result = FALSE, return_result = TRUE)
```

The method arguments are:

- `initial = "random"` for random starting values drawn from a standard normal distribution,

- `which_optimizer = "nlm"` for optimization with the above specified `stats::nlm` optimizer,

- `save_result = FALSE` to not save the optimization result in the `geyser` object (see below),

- and `return_results = TRUE` to return the optimization result.

The return value is a `list` of:

- `value`, the optimum function value,

- `parameter`, the parameter vector where `value` is obtained,

- `seconds`, the estimation time in seconds,

- `initial`, the starting parameter vector for the optimization,

- and `gradient`, `code`, and `iterations`, which are outputs specific to the `stats::nlm` optimizer.

### Initialization effect

We are interested in the effect of the starting values on the optimization, i.e., whether different initial values lead to different results. We therefore optimize the likelihood function `runs = 100` times with different random starting points and compare the identified optima:

```{r}
geyser$optimize(initial = "random", runs = 100, label = "random", save_results = TRUE, seed = 1)
```

We label the optimization results with `label = "random"`, which is useful later for comparison. We set `save_results = TRUE` to save the optimization results inside the `geyser` object (so that we can use the `$optima()`, `$summary()`, and `$plot()` methods for comparisons, see below). The `seed = 1` argument ensures reproducibility.

The `$optima()` method provides an overview of the identified optima. We see below that the different runs led to `r nrow(geyser$optima(digits = 0, sort_by = "value"))` different optima (minima in this case, because we minimize `normal_mixture_llk()`), hence the initial starting values have a hugh impact on the optimization result.

```{r}
geyser$optima(digits = 0, sort_by = "value")
```

TODO: extract and compare parameters that led to different minima.

### Custom sampler for initial values

Depending on the application and the magnitude of the parameters to be estimated, initial values drawn from a standard normal distribution (which is the default behavior when calling `$optimize(initial = "random")`) may not be a good guess. We can, however, easily modify the distribution that is used to draw the random numbers. For example, the next code snippet uses starting values drawn from a $\mathcal{N}(2, 0.5)$ distribution:

```{r}
sampler <- function() stats::rnorm(5, mean = 2, sd = 0.5)
geyser$optimize(initial = sampler, runs = 100, label = "custom_sampler")
```

### Educated guesses

Next we make "educated guesses" about starting values that are probably close to the global optimum. Based on the histogram above, the means of the two normal distributions may be somewhere around 2 and 4. For the variances, we set the starting values to 1 (note that we use the log transformation here since we restrict the standard deviations to be positive by using `exp()` in the log-likelihood function). 

We will use two sets of starting values where the means are lower and larger than 2 and 4, respectively. For both sets, the starting value for the mixing proportion is 0.5.

```{r}
starting_values <- list(c(1.7, 4.3, log(1), log(1), qlogis(0.5)),
                        c(2.3, 3.7, log(1), log(1), qlogis(0.5)))
```

For comparison, we also consider a third set of starting values which are somewhat implausible.

```{r}
starting_values[[3]] <- c(10, 8, log(0.1), log(0.2), qlogis(0.5))
```

In the `$optimize()` method, instead of `label = "random"`, we can set `label` to a numeric vector of length `npar`, or for convenience to a `list` of such vectors, like `starting_values`:

```{r}
geyser$optimize(initial = starting_values, label = "educated_guess")
```

Let's look at the results:

```{r}
summary(geyser, which_runs = "last")
```

TODO: apply round to reduce decimal places

The `summary()` method returns an overview of the three optimization runs performed with the two optimizer.

TODO: Show more values with the `"comment"` argument. See names of all available columns with `"geyser$summary_columns"`.

```{r}
# geyser$get_value(which_value = "initial", which_run = 10, which_optimizer = 100)
```

### Discard optimization runs

The third run (with the unplausible starting values) converged to a local minimum, as we can deduce from the column "value". We discard this run from further comparisons:

TODO

```{r}
# geyser$clear(which_runs = 103, which_optimizer = "all")
```

### Standardized initialization

For standardized initialization, we standardize all columns in our data before running the optimization. The method `$standardize()` ... TODO

```{r}
geyser$standardize("data")
geyser$optimize(runs = 10, label = "standardize")
geyser$reset_argument("data")
```

### Subset initialization

If the data set considered shows some complex structures or is very large, numerical optimization may become computationally costly. In such cases, subset initialization can be a helpful tool. 

The method `$reduce()` ... TODO

```{r}
geyser$reduce("data", how = "random", prop = 0.25)
geyser$optimize(runs = 10, label = "subset")$continue()
geyser$reset_argument("data")
```

In addition to selecting sub samples at random, we can specify two further options using the argument `how`. When dealing with time series data, we usually do not want to delete single observations at random. Instead, we can select a proportion of the first rows by specifying `how = "first"`. We could also cluster our data first using `how = "kmeans"`. 

### Evaluating the optimization runs

The `plot()` method provides an overview of the optimization times. 

```{r, out.width = "50%"}
geyser$plot()
```

We can compare the optimization times across initialization strategies by specifying `by = label`:

```{r}
geyser$plot(by = "label", nrow = 1)
```

TODO: sort boxplots by their median in descending order

## References
