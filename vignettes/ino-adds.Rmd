## part 1

The two most occurring optima are 421 and 276 with total frequencies of ... and ..., respectively. The value 276 is the overall minimum (potentially the global minimum), while 421 is significantly worse.

To compare the parameter vectors that led to these different values, we can use the `$closest_parameter()` method. From the saved optimization runs, it extracts the parameter vector corresponding to an optimum closest to `value`. We consider only results from the `nlm` optimizer here: 

```{r, closest parameters}
(mle <- Nop_mixture$closest_parameter(value = 276, which_run = "random", which_optimizer = "nlm"))
Nop_mixture$evaluate(at = as.vector(mle))
mle_run <- attr(mle, "run")
(bad <- Nop_mixture$closest_parameter(value = 421, which_run = "random", which_optimizer = "nlm"))
Nop_mixture$evaluate(at = as.vector(bad))
bad_run <- attr(bad, "run")
```

These two parameter vectors are saved as `mle` (this shall be our maximum likelihood estimate) and `bad` (this clearly is a bad estimate). Two attributes show the run id and the optimizer that led to these parameters.

To understand the values in terms of means, standard deviations, and mixing proportion (i.e., in the form $\boldsymbol{\theta} = (\mu_1, \mu_2, \sigma_1, \sigma_2, \lambda)$), they need transformation (see above):

```{r, transform parameter}
transform <- function(theta) c(theta[1:2], exp(theta[3:4]), plogis(theta[5]))
(mle <- transform(mle))
(bad <- transform(bad))
```

The two estimates `mle` and `bad` for $\boldsymbol{\theta}$ correspond to the following mixture densities:

```{r, estimated-mixtures}
mixture_density <- function (data, mu, sd, lambda) {
  lambda * dnorm(data, mu[1], sd[1]) + (1 - lambda) * dnorm(data, mu[2], sd[2])
}
ggplot(faithful, aes(x = eruptions)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30) + 
  labs(x = "eruption time (min)", colour = "parameter") +
  stat_function(
    fun = function(x) {
      mixture_density(x, mu = mle[1:2], sd = mle[3:4], lambda = mle[5])
    }, aes(color = "mle"), linewidth = 1
  ) +
  stat_function(
    fun = function(x) {
      mixture_density(x, mu = bad[1:2], sd = bad[3:4], lambda = bad[5])
    }, aes(color = "bad"), linewidth = 1
  )
```

The mixture defined by the `mle` parameter fits much better than `bad`, which practically estimates only a single class. However, the gradients at both points are close to zero, which explains why the `nlm` optimizer terminates at both points:

```{r, extract gradients}
Nop_mixture$results(
  which_run = c(mle_run, bad_run), which_optimizer = "nlm", which_element = "gradient"
)
```

### Educated guesses

Next we make "educated guesses" about starting values that are probably close to the global optimum. Based on the histogram above, the means of the two normal distributions may be somewhere around 2 and 4. We will use sets of starting values where the means are lower and larger than 2 and 4, respectively. For the variances, we set the starting values close to 1 (note that we use the log transformation here since we restrict the standard deviations to be positive by using `exp()` in the log-likelihood function). The starting value for the mixing proportion shall be around 0.5. This leads to the following 32 combinations of starting values:

```{r, fixed starting values}
mu_1 <- c(1.7, 2.3)
mu_2 <- c(4.3, 3.7)
sd_1 <- sd_2 <- c(log(0.8), log(1.2))
lambda <- c(qlogis(0.4), qlogis(0.6))
starting_values <- asplit(expand.grid(mu_1, mu_2, sd_1, sd_2, lambda), MARGIN = 1)
```

In the `$optimize()` method, instead of `initial = "random"`, we can set `initial` to a numeric vector of length `npar`, or, for convenience, to a `list` of such vectors, like `starting_values`:

```{r, optimization with educated guesses}
Nop_mixture$optimize(initial = starting_values, label = "educated_guess")
```

These "educated guesses" lead to a way more stable optimization:

```{r, overview optima for educated guesses}
Nop_mixture$optima(digits = 0, which_run = "educated_guess")
```

For comparison, we consider a set of implausible starting values...

```{r, bad guess}
Nop_mixture$optimize(initial = rep(0, 5), label = "bad_guess")
```

... which lead to local optima:

```{r, bad guess summary, which_run = "random"}
summary(Nop_mixture, which_run = "bad_guess") 
```

### Standardizing the optimization problem

In some situations, it is possible to consider a standardized version of the optimization problem, which could potentially improve the performance of the numerical optimizer. In our example, we can standardize the data before running the optimization via the `$standardize()` method:

```{r, standardize data}
Nop_mixture$standardize("data")
str(Nop_mixture$get_argument("data"))
```

To optimize the likelihood using the standardized data set, we again use `$optmize()`, which by default uses random starting values. Below, we will compare these results with those obtained on the original optimization problem. 

```{r, optimization with standardized data}
Nop_mixture$
  optimize(runs = 100, label = "data_standardized")$
  reset_argument("data")
```

The usage of `$reset_argument("data")` is important: to perform further optimization runs after having applied standardized initialization, we undo the standardization of the data and obtain the original data set. If we would not use `$reset_argument()`, all further optimization runs will be carried out on the standardized data set. 

### Reducing the optimization problem

In some situations, it is possible to first optimize a sub-problem and use those results as an initialization for the full optimization problem. For example, in the context of likelihood maximization, if the data set considered shows some complex structures or is very large, numerical optimization may become computationally costly. In such cases, it can be beneficial to initially consider a reduced data set. The following application of the `$reduce()` method transforms `"data"` by selecting a proportion of 30\% data points at random:

```{r, reduce data}
Nop_mixture$reduce(argument_name = "data", how = "random", prop = 0.3, seed = 1)
str(Nop_mixture$get_argument("data"))
```

Similar to the standardizing above, calling `$optimize()` now optimizes on the reduced data set:

```{r, optimization with reduced data}
Nop_mixture$
  optimize(runs = 100, label = "data_subset")$
  reset_argument("data")$
  continue()
```

Again, we use `$reset_argument("data")` to obtain the original data set. The `$continue()` method now optimizes on the whole data set using the estimates obtained on the reduced data as initial values.

In addition to selecting sub samples at random (`how = "random"`), four other options exist via specifying the argument `how`:

- `"first"` selects the top data points,
- `"last"` selects the last data points,
- `"similar` selects similar data points based on k-means clustering,
- `"dissimilar"` is similar to `"similar"` but selects dissimilar data points.

### Optimization times

The `$plot()` method provides an overview of the optimization times. Setting `by = "label"` allows for comparison across initialization strategies, setting `relative = TRUE` plots relative differences to the median of the top boxplot:

```{r, plot-by-label}
Nop_mixture$plot(by = "label", relative = TRUE, xlim = c(-1, 3))
```

Setting `by = "optimizer"` allows comparison across optimizers:

```{r, plot-by-optimizer}
Nop_mixture$plot(by = "optimizer", relative = FALSE, xlim = c(0, 0.05))
```

## References
