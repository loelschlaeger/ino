---
title: "Example: Probit Model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Example: Probit Model}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.dim = c(8, 6), 
  out.width = "75%"
)
library("ino")
options(ino_progress = FALSE)
```

The probit model is a flexible and widely-used tool for the analysis of such discrete choice behavior. Choosing between alternatives is omnipresent in everyday life, from the choice of a vehicle for traveling to work over different brands in a supermarket to companies deciding among production plans. Many scientific areas apply the probit model for studying the driving factors behind decision makers' choices, for example transportation [@Bolduc:1999; @Shin:2015] and marketing [@Allenby:1998; @Haaijer:1998; @Paap:2000]. Estimating the probit model's parameters traditionally is performed via maximizing the likelihood function numerically. With rising model complexity however, this approach becomes both computationally expensive and does not guarantee convergence to the global optimum.  

## The model formulation
	
We briefly formulate the probit model and its estimation and refer to @Train:2009 and @Bhat:2011 for further details. Say that $N$ deciders choose among $J \geq 2$ alternatives at each of $T$ choice occasions. The values for $J$ and $T$ can be decider-specific, though we do not show this dependence in our notation. Let $y_{nt} \in \{1,\dots,J\}$ label the choice of decider $n$ at occasion $t$. Assume that the choice was rational in the sense that $y_{nt}$ yields the highest utility $U_{nt}$ for $n$ at $t$. The probit model defines $U_{nt} = X_{nt} \beta + \epsilon_{nt}$, where $X_{nt}$ is a $J\times P$-matrix of $P$ characteristics for each alternative, $\beta$ is a coefficient vector of length $P$ and $\epsilon_{nt} \sim N(0,\Sigma)$ denotes the vector of jointly normal distributed unobserved influences. We ensure identifiability by taking utility differences and fixing one error-term variance. This implies that instead of $\Sigma$, we estimate $J(J-1)/2-1$ parameters of a transformed covariance matrix. 
	
The researcher aims to estimate the values for $b$ and $\Sigma$, most commonly by the maximum likelihood method. Let $\theta$ denote the vector of the $P$ coefficients of $b$ and $J(J-1)/2-1$ identified parameters of $\Sigma$. Note that the length of $\theta$ rises quadratically with $J$. The maximum likelihood estimate $\hat{\theta}$ is obtained by solving
\begin{equation}
	\label{eq:ll}
	\arg \max_\theta \log \sum_{n,t,j} 1(y_{nt} = j) \int 1(j = \arg \max U_{nt}) \phi(\epsilon_{nt}) d \epsilon_{nt},
\end{equation}
where $1(\cdot)$ denotes the indicator function and $\phi(\cdot)$ the normal density. The integral part of does not have a closed-form expression and hence must be approximated numerically.
	
## Initializing with standardizing
	
## Initializing using a subsample
	
## Alternating optimization
	
## References

