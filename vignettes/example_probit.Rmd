---
title: "Example: Probit Model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Example: Probit Model}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.dim = c(8, 6), 
  out.width = "75%",
  eval = FALSE
)
# library("ino")
devtools::load_all() # remove later
options("ino_verbose" = FALSE)
```

The probit model is widely used to study discrete choice behavior in fields like transportation [@Bolduc:1999; @Shin:2015] and marketing [@Allenby:1998; @Haaijer:1998; @Paap:2000]. Typically, estimating the probit model's parameters involves numerically maximizing the likelihood function. This approach, however, can be computationally expensive and may not converge to the global optimum, especially for complex models. In this vignette, we use the {ino} package to investigate the effect of initialization on numerical probit likelihood maximization.

## Model formulation

We briefly formulate the probit model and its estimation. For more details, see @Train:2009 and @Bhat:2011. Assume that $N$ deciders choose among $J \geq 2$ alternatives at each of $T$ choice occasions. Let $y_{nt} \in \{1,\dots,J\}$ label the choice of decider $n$ at occasion $t$. Assume further that the choices are rational in the sense that $y_{nt}$ corresponds to the largest entry in the utility vector $U_{nt} \in \mathbb{R}^J$ for $n$ at $t$. 

The probit model defines $$U_{nt} = X_{nt} b + \epsilon_{nt},$$ where $X_{nt}$ is a $J\times P$ matrix of $P$ characteristics for each alternative, $b$ is a coefficient vector of length $P$, and $\epsilon_{nt} \sim N(0,\Sigma)$ denotes the vector of jointly normally distributed errors (i.e., unobserved influences on the utility). 

The probit model (like any utility model) is invariant to the level and scale of the utilities $U_{nt}$. We ensure identifiability by considering utility differences, which reduces $\Sigma$ from $J$ to $J-1$ dimensions, and fixing the first entry of $b$ to $1$.

To account for preference heterogeneity across deciders, the mixed probit model includes decider-specific coefficient vectors as $\beta_n \sim N(b, \Omega)$. In the degenerate case $\Omega = 0$, we have $\beta_n = b$, i.e., all deciders share the same preferences.
	
The researcher aims to estimate the values for $b$, $\Omega$, and $\Sigma$, given a set of observed choice data. The most common approach is the maximum likelihood method. Let $\theta$ denote the vector of the identified parameters, i.e., $P-1$ coefficients of $b$, $P(P+1)/2$ coefficients of $\Omega$, and $J(J-1)/2$ coefficients of the differenced matrix $\Sigma$. To ensure that the estimates result in proper covariance matrices, we actually optimize over the Cholesky factors. Note that the length of the parameter vector $\theta$ increases quadratically with both the number of alternatives $J$ and the number of choice characteristics $P$. This quadratic increase indicates that numerical optimization becomes computationally demanding for complex models with a high number of choice alternatives or choice covariates.

The maximum likelihood estimate $\hat{\theta}$ is obtained by solving $$\hat{\theta} = \arg \max_\theta \log \sum_{n,t,j} 1(y_{nt} = j) \int 1(j = \arg \max U_{nt}) \phi(\epsilon_{nt}) d \epsilon_{nt},$$
where $1(\cdot)$ denotes the indicator function and $\phi(\cdot)$ the Gaussian density. The integral part of the equation does not have a closed-form expression, and therefore, it must be approximated numerically. Here, we apply the `mvtnorm::GenzBretz` algorithm [@Genz:2009].

## Data simulation and likelihood computation

The {ino} package provides the `sim_mnp()` function for simulating choice data from a probit model. To use this function, we first need to define a function `X()` that samples the matrix of choice characteristics for the decision maker $n$ at choice occasion $t$. The matrix must have the dimension $J \times P$, where $J$ is the number of available alternatives and $P$ the number of characteristics that describe each alternative.

In our simulation, the choice setting consists of $J = 3$ alternatives that are described via $P = 2$ characteristics, and each entry of $X_{nt}$ is drawn from a $\mathcal{N}(\mu = 0, \sigma = 3)$ distribution:

```{r, choice covariates, eval = TRUE}
J <- 3
P <- 2
X <- function(n, t) {
  matrix(stats::rnorm(J * P, mean = 0, sd = 3), nrow = J)
}
X(n = 1, t = 1)
```

We simulate choice data for $N = 200$ deciders at $T = 20$ choice occasions from the probit model defined by the parameters $b = \begin{pmatrix} 1 & -1 \end{pmatrix}^\top$, $\Omega = \begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$, and $\Sigma = \begin{pmatrix} 1 & -0.5 & 0.25 \\ -0.5 & 1 & 0.25 \\ 0.25 & 0.25 & 1 \end{pmatrix}$:

```{r, simulate data, eval = TRUE}
N <- 200
T <- 20
b <- c(1, -1)
Omega <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
Sigma <- matrix(c(1, -0.5, 0.25, -0.5, 1, 0.25, 0.25, 0.25, 1), 3, 3)
probit_data <- sim_mnp(N, T, J, P, b, Omega, Sigma, X, seed = 1)
```

The `probit_data` object is a `data.frame` with the decider index `n`, the choice occasion index `t`, the choice `y`, and the choice characteristics `X`:

```{r, head of data, eval = TRUE}
head(probit_data)
```

It has the attribute `true`, which provides the true and *identified* parameter values, i.e. the mean effects $b$ except for the first one, the elements $o$ of the lower-triangular Cholesky root of $\Omega$, and the elements $l$ of the lower-triangular Cholesky root of the differenced (with respect to alternative $J$) covariance matrix $\Sigma$:

```{r, true parameter, eval = TRUE}
(theta <- attr(probit_data, "true"))
```

The likelihood function is implemented as `f_ll_mnp()`:

```{r, likelihood evaluation, eval = TRUE}
f_ll_mnp(theta = theta, data = probit_data)
```

## Setup in {ino}

To analyze the outcome of the numerical likelihood optimization for the multinomial mixed probit model, we apply the {ino} package. We first define a `Nop` object by setting `f = f_ll_mnp`, the number of parameters to `npar = 7`, and `data = probit_data`. Then, we apply the `stats::nlm` optimizer with a limit of 1000 iterations. Since this optimizer minimizes the objective function instead of maximizing it, we set `neg = TRUE` to compute the negative log-likelihood value.

```{r, define Nop, eval = TRUE}
probit_ino <- Nop$new(f = f_ll_mnp, npar = 7, data = probit_data, neg = TRUE)$
  set_optimizer(optimizer_nlm(iterlim = 1000))
```

We save the true parameter vector `theta` and use it later to evaluate whether the optimization has converged to the global optimum:

```{r, set true parameter, eval = TRUE}
probit_ino$true_parameter <- theta
```

The initial `Nop` object looks as follows:

```{r, print initial Nop object, eval = TRUE}
print(probit_ino)
```

## Random initialization

We optimize `runs = 100` times using random initial values drawn from a standard normal distribution:

```{r, optimize with random initial values}
probit_ino$optimize(initial = "random", runs = 100, label = "random", ncores = 4)
```

## Initializing using a subsample

We initialize on a subset of proportion 20\% and 50\%, which was selected randomly and using kmeans, respectively.

```{r}
for(how in c("random", "kmeans")) for(prop in c(0.2,0.5)) {
  probit_ino <- subset_initialization(
    probit_ino, arg = "data", how = how, prop = prop,
    ind_ign = 1:3, initialization = random_initialization(runs = 100)
  )
}
```

## Remove runs that did not converge

3 optimization runs reached the iteration limit of 1000 iterations:

```{r, eval = FALSE}
library("dplyr", warn.conflicts = FALSE)
summary(probit_ino, "iterations" = "iterations") %>% filter(iterations >= 1000)
```

We exclude them from further analysis:

```{r, eval = FALSE}
ind <- which(summary(probit_ino, "iterations" = "iterations")$iterations >= 1000)
probit_ino <- clear_ino(probit_ino, which = ind) 
```

## Comparison

```{r, out.width = "100%", fig.dim = c(10, 6), eval = FALSE}
plot(probit_ino, by = ".strategy", time_unit = "mins", nrow = 1)
```

We see that the subset initialization strategies reduce the computation time significantly, in comparison to the random initialization on the full data set.
	
## References

