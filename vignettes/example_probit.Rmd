---
title: "Example: Probit Model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Example: Probit Model}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.dim = c(8, 6), 
  out.width = "75%",
  eval = FALSE
)
# library("ino")
devtools::load_all() # remove later
options("ino_verbose" = FALSE)
```

The probit model is widely used to study discrete choice behavior in fields like transportation [@Bolduc:1999; @Shin:2015] and marketing [@Allenby:1998; @Haaijer:1998; @Paap:2000]. Typically, estimating the probit model's parameters involves numerically maximizing the likelihood function. This approach, however, can be computationally expensive and may not converge to the global optimum, especially for complex models. In this vignette, we use the {ino} package to investigate the effect of initialization on numerical probit likelihood maximization.

## Model formulation

We briefly formulate the probit model and its estimation. For more details, see @Train:2009 and @Bhat:2011. Assume that $N$ deciders choose among $J \geq 2$ alternatives at each of $T$ choice occasions. Let $y_{nt} \in \{1,\dots,J\}$ label the choice of decider $n$ at occasion $t$. Assume further that the choices are rational in the sense that $y_{nt}$ corresponds to the largest entry in the utility vector $U_{nt} \in \mathbb{R}^J$ for $n$ at $t$. 

The probit model defines $$U_{nt} = X_{nt} b + \epsilon_{nt},$$ where $X_{nt}$ is a $J\times P$ matrix of $P$ characteristics for each alternative, $b$ is a coefficient vector of length $P$, and $\epsilon_{nt} \sim N(0,\Sigma)$ denotes the vector of jointly normally distributed errors (i.e., unobserved influences on the utility). 

The probit model (like any utility model) is invariant to the level and scale of the utilities $U_{nt}$. We ensure identifiability by considering utility differences, which reduces $\Sigma$ from $J$ to $J-1$ dimensions, and fixing the first entry of $b$ to $1$.

To account for preference heterogeneity across deciders, the mixed probit model includes decider-specific coefficient vectors as $\beta_n \sim N(b, \Omega)$. In the degenerate case $\Omega = 0$, we have $\beta_n = b$, i.e., all deciders share the same preferences.
	
The researcher aims to estimate the values for $b$, $\Omega$, and $\Sigma$, given a set of observed choice data. The most common approach is the maximum likelihood method. Let $\theta$ denote the vector of the identified parameters, i.e., $P-1$ coefficients of $b$, $P(P+1)/2$ coefficients of $\Omega$, and $J(J-1)/2$ coefficients of the differenced matrix $\Sigma$. Note that the length of the parameter vector $\theta$ increases quadratically with both the number of alternatives $J$ and the number of choice characteristics $P$. This quadratic increase indicates that numerical optimization becomes computationally demanding for complex models with a high number of choice alternatives or choice covariates.

The maximum likelihood estimate $\hat{\theta}$ is obtained by solving $$\hat{\theta} = \arg \max_\theta \log \sum_{n,t,j} 1(y_{nt} = j) \int 1(j = \arg \max U_{nt}) \phi(\epsilon_{nt}) d \epsilon_{nt},$$
where $1(\cdot)$ denotes the indicator function and $\phi(\cdot)$ the Gaussian density. The integral part of the equation does not have a closed-form expression, and therefore, it must be approximated numerically. Here, we apply the `mvtnorm::GenzBretz` algorithm [@Genz:2009].

## Data simulation and likelihood computation

To simulate data from a probit model, the {ino} package offers the `sim_mnp()` function. We first define a function `X` that samples the $J\times P$ matrix of choice characteristics of decider $n$ at choice occasion $t$. 

```{r}
J <- 3
P <- 3
scale <- c(1, 1e-2, 1e+2)
X <- function(n, t) {
  class <- sample(0:1, 1)
  mean <- ifelse(class, 1, -1)
  covariates <- stats::rnorm(J * P, mean = mean, sd = 1) / scale
  matrix(covariates, nrow = J, ncol = P, byrow = TRUE)
}
X(1, 1)
```

```{r}
N <- 100
T <- 10
b <- c(1, -1, 2) * scale
Omega <- diag(P)
Sigma <- diag(J)
probit_data <- sim_mnp(N, T, J, P, b, Omega, Sigma, X)
```

```{r, eval = FALSE}
theta <- attr(probit_data, "true")[-1]
f_ll_mnp(theta = theta, data = probit_data, neg = TRUE)
nlm(f_ll_mnp, p = theta, data = probit_data, neg = TRUE, print.level = 2)$estimate
```

## Setup in {ino}

The following lines specify the `ino` object. The likelihood is computed via `f_ll_mnp()` which is provided via {ino}.

```{r}
probit_ino <- Nop$new(
  f = f_ll_mnp,
  npar = length(theta),
  data = probit_data,
  neg = TRUE
)
```

```{r}
probit_ino$set_optimizer(optimizer_nlm(iterlim = 1000))
```

```{r}
probit_ino$true_parameter <- theta
```

```{r}
probit_ino$test(at = theta)
```

## Random initialization

We initialize `runs = 100` times randomly.

```{r}
probit_ino$optimize(initial = "random", runs = 100)
```

## Initializing using a subsample

We initialize on a subset of proportion 20\% and 50\%, which was selected randomly and using kmeans, respectively.

```{r}
for(how in c("random", "kmeans")) for(prop in c(0.2,0.5)) {
  probit_ino <- subset_initialization(
    probit_ino, arg = "data", how = how, prop = prop,
    ind_ign = 1:3, initialization = random_initialization(runs = 100)
  )
}
```

## Remove runs that did not converge

3 optimization runs reached the iteration limit of 1000 iterations:

```{r, eval = FALSE}
library("dplyr", warn.conflicts = FALSE)
summary(probit_ino, "iterations" = "iterations") %>% filter(iterations >= 1000)
```

We exclude them from further analysis:

```{r, eval = FALSE}
ind <- which(summary(probit_ino, "iterations" = "iterations")$iterations >= 1000)
probit_ino <- clear_ino(probit_ino, which = ind) 
```

## Comparison

```{r, out.width = "100%", fig.dim = c(10, 6), eval = FALSE}
plot(probit_ino, by = ".strategy", time_unit = "mins", nrow = 1)
```

We see that the subset initialization strategies reduce the computation time significantly, in comparison to the random initialization on the full data set.
	
## References

