---
title: "Example: Probit Model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Example: Probit Model}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.dim = c(8, 6), 
  out.width = "75%",
  cache = TRUE
)
# library("ino")
devtools::load_all() # remove later
options("ino_verbose" = FALSE)
```

The probit model is widely used to study discrete choice behavior in fields like transportation [@Bolduc:1999; @Shin:2015] and marketing [@Allenby:1998; @Haaijer:1998; @Paap:2000]. Typically, estimating the probit model's parameters involves numerically maximizing the likelihood function. However, this approach can be computationally expensive and may not converge to the global optimum, especially for complex models. In this vignette, we use the {ino} package to investigate the effect of initialization on probit likelihood maximization.

## Model formulation

We briefly formulate the probit model and its estimation. For more details, see @Train:2009 and @Bhat:2011. Assume that $N$ deciders choose among $J \geq 2$ alternatives at each of $T$ choice occasions. Let $y_{nt} \in \{1,\dots,J\}$ label the choice of decider $n$ at occasion $t$. Assume further that the choices are rational in the sense that $y_{nt}$ yields the highest utility $U_{nt}$ for $n$ at $t$. The probit model defines $$U_{nt} = X_{nt} \beta + \epsilon_{nt},$$ where $X_{nt}$ is a $J\times P$ matrix of $P$ characteristics for each alternative, $\beta$ is a coefficient vector of length $P$, and $\epsilon_{nt} \sim N(0,\Sigma)$ denotes the vector of jointly normal distributed errors (i.e., unobserved influences on the utility). 

The probit model (like any utility model) is invariant to the level and scale of the utilities $U_{nt}$. We ensure identifiability by considering utility differences, which reduces $\Sigma$ to $J-1$ dimensions, and fixing the first entry of $b$ to $1$.

To account for preference heterogeneity across deciders, the mixed probit model includes decider-specific coefficient vectors as $\beta_n \sim N(b, \Omega)$. In the degenerate case $\Omega = 0$, we have $\beta_n = b$, i.e., all deciders share the same preferences.
	
The researcher aims to estimate the values for $b$, $\Omega$, and $\Sigma$, most commonly by the maximum likelihood method. Let $\theta$ denote the vector of the identified parameters, i.e., $P-1$ coefficients of $b$, $P(P+1)/2$ coefficients of $\Omega$, and $J(J-1)/2$ coefficients of the differenced matrix $\Sigma$. Note that the length of $\theta$ rises quadratically with both number of alternatives $J$ and choice characteristics $P$. 

The maximum likelihood estimate $\hat{\theta}$ is obtained by solving $$\arg \max_\theta \log \sum_{n,t,j} 1(y_{nt} = j) \int 1(j = \arg \max U_{nt}) \phi(\epsilon_{nt}) d \epsilon_{nt},$$
where $1(\cdot)$ denotes the indicator function and $\phi(\cdot)$ the normal density. The integral part of the equation does not have a closed-form expression, and therefore, it must be approximated numerically.

## Data simulation and likelihood computation

To simulate data from a probit model, the {ino} package offers the `sim_mnp()` function. We first define a function `X` that samples the $J\times P$ matrix of choice characteristics of decider $n$ at choice occasion $t$. 

```{r}
J <- 3
P <- 4
scale <- c(1e-4, 1e-2, 1, 1e+4)
X <- function(n, t) {
  class <- sample(0:1, 1)
  mean <- ifelse(class, 1, -1)
  covariates <- stats::rnorm(J * P, mean = mean, sd = 1) / scale
  matrix(covariates, nrow = J, ncol = P, byrow = TRUE)
}
X(1, 1)
```

```{r}
N <- 100
T <- 10
b <- c(1, -1, 2, 0.5)
Omega <- diag(P)
Sigma <- diag(J)
probit_data <- sim_mnp(N, T, J, P, b, Omega, Sigma, X)
```

```{r}
theta <- attr(probit_data, "true")[-1]
f_ll_mnp(theta = theta, data = probit_data, neg = TRUE)
# nlm(f_ll_mnp, p = theta, data = data, neg = TRUE)$estimate
```

## Setup

The following lines specify the `ino` object. The likelihood is computed via `f_ll_mnp()` which is provided via {ino}. Via the `global` argument, we can specify the true parameter vector that leads to the global optimum. The `mpvs = "data"` input specifies that we want to loop over the ten provided data sets.

```{r, eval = FALSE}
probit_ino <- Nop$new(
  f = f_ll_mnp,
  npar = length(theta),
  data = probit_data,
  neg = TRUE)$
  set_optimizer(optimizer_nlm(iterlim = 1000))$
  set_true_parameter(theta, set_true_value = TRUE)
```

```{r}
probit_ino$test(at = theta)
```

## Random initialization

We initialize `runs = 100` times randomly.

```{r, eval = FALSE}
probit_ino$optimize(initial = "random", runs = 100)
```

## Initializing using a subsample

We initialize on a subset of proportion 20\% and 50\%, which was selected randomly and using kmeans, respectively.

```{r, eval = FALSE}
for(how in c("random", "kmeans")) for(prop in c(0.2,0.5)) {
  probit_ino <- subset_initialization(
    probit_ino, arg = "data", how = how, prop = prop,
    ind_ign = 1:3, initialization = random_initialization(runs = 100)
  )
}
```

## Remove runs that did not converge

3 optimization runs reached the iteration limit of 1000 iterations:

```{r, eval = FALSE}
library("dplyr", warn.conflicts = FALSE)
summary(probit_ino, "iterations" = "iterations") %>% filter(iterations >= 1000)
```

We exclude them from further analysis:

```{r, eval = FALSE}
ind <- which(summary(probit_ino, "iterations" = "iterations")$iterations >= 1000)
probit_ino <- clear_ino(probit_ino, which = ind) 
```

## Comparison

```{r, out.width = "100%", fig.dim = c(10, 6), eval = FALSE}
plot(probit_ino, by = ".strategy", time_unit = "mins", nrow = 1)
```

We see that the subset initialization strategies reduce the computation time significantly, in comparison to the random initialization on the full data set.
	
## References

