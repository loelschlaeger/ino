---
title: "Introduction"
description: >
  Learn how to get started with the {ino} package.
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse  = TRUE,
  comment   = "#>",
  fig.align = "center",
  fig.path  = "figures/ino-",
  fig.dim   = c(8, 6), 
  out.width = "75%"
)
```

## Motivation

Optimization refers to making something as effective, efficient, or functional as possible. This goal is important in many fields, for example optimal portfolio composition in finance, minimizing air resistance in engineering, or likelihood maximization for model fitting in statistics. In any case, assuming a function can be defined that maps inputs to outputs, the goal of optimization is to find inputs such that the output is in some sense optimal.

In some cases, the function is simple enough that finding the optimal value is possible analytically: for example, if $f:\mathbb{R} \to \mathbb{R},\ f(x) = -x^2$, we find that the first derivative $f'(x) = -2x$ vanishes at $x = 0$, and since $f$ is strictly concave, this $x$ must be the unique point where $f(x)$ is maximal. But often, the optimization problem at hand cannot be solved analytically, for example when explicit formulas for derivatives are unknown or do not exist. In these cases, optimization is still possible via a concept called numerical optimization.

Numerical optimization refers to any algorithm that iteratively explores the parameter space, guaranteeing to improve the function output over each iteration, and eventually converging to a point where no more improvements can be made [@Bonnans:2006]. Several such algorithms are implemented in R.^[The CRAN Task View: Optimization and Mathematical Programming [@Schwendinger:2023] provides a comprehensive list of packages for solving optimization problems numerically.] One thing that all of these algorithms have in common is that initial parameter values must be specified, i.e., the point from where the procedure is started. But choosing initial values is non-trivial and can be a challenging task.

The reason why initialization is so challenging is that starting values can have a large influence on the optimization time and optimization result [@Nocedal:2006]. In general, starting in areas of function saturation increases computation time, starting in areas of non-concavity leads to convergence problems or convergence to local rather than global optima. The increase in computation time is a problem for complex functions where each iteration of the optimizer takes a significant evaluation time, while the concern of convergence issues to the global optimum is problematic in any case. So two questions immediately arise:

1. Is my optimization problem affected by initialization?

2. If so, what are good initial values in the sense that optimization is fast and leads to the global optimum?

## Package functionality

We introduce the `{ino}` R package (acronym for initialization of numerical optimization), which helps in finding answers to the questions above. In particular, the package provides tools for three tasks:

1. exploring the effect of the initial values on the optimization

2. comparing different initialization strategies

3. comparing different numerical optimizers

The package follows an object-oriented approach^[We make use of the object-oriented framework of the `{R6}` package [@Chang:2021].], treating numerical optimization problems as objects. The object is defined by a real-valued function, the argument over which the function is to be optimized, and one or more optimization algorithms. The object provides methods for choosing initial values, carrying out numerical optimization, and evaluating the optimization results.

The main benefits of using the package are as follows:

- Any optimizer implemented in R can be applied for minimization and maximization tasks. To achieve consistency across implementations of optimization algorithms in R, we make use of the framework provided by the `{optimizeR}` package [@Oelschlaeger:2023]. Below in the appendix, we provide more details on this framework.

- Generally applicable initialization strategies are implemented, and users can extend custom strategies.

- Methods allow for easy comparisons across optimizers and initialization strategies, and to finally extract the best optimization result.

- The package supports parallel computation and progress updates.

- We put effort into a user-friendly workflow: every input is documented in detail, every user input is validated, and users only interact with a single object. 

## Example workflow

To illustrate the package workflow, we solve a likelihood optimization problem. In a first step, `{ino}` can be obtained from [CRAN](https://CRAN.R-project.org) via:

```{r, load ino}
# install.packages("ino")
# library("ino")
rm(list = ls())      # TODO: remove
devtools::load_all() # TODO: remove
```

### Gaussian mixture model

The function to be optimized in this example is a likelihood function, which computes the probability of observing given data under a model assumption. The parameters that maximize the likelihood function are then identified as the model estimates. This approach is called maximum likelihood estimation and is very popular in statistics for fitting a model to empirical data.

We consider eruption times of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The data histogram hints at two clusters with short and long eruption times, respectively:

```{r, faithful plot, warning = FALSE}
library("ggplot2")
ggplot(faithful, aes(x = eruptions)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30) + 
  xlab("eruption time (min)") 
```

For both clusters, we assume a normal distribution, such that we consider a mixture of two Gaussian densities for modeling the overall eruption times. The log-likelihood function^[Optimizing the log-likelihood function is equivalent to optimizing the likelihood function, because likelihoods are positive and the logarithm is a monotone transformation. But optimizing the log-likelihood function has numerical advantages, in particular avoiding numerical underflow when the likelihood becomes small.] is defined by

\begin{equation}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \log\Big( \lambda \phi_{\mu_1, \sigma_1^2}(x_i) + (1-\lambda)\phi_{\mu_2,\sigma_2^2} (x_i) \Big),
\end{equation}

where the sum goes over all observations $1, \dots, n = 272$, $\phi_{\mu_1, \sigma_1^2}$ and $\phi_{\mu_2, \sigma_2^2}$ denote the normal density for the first and second cluster, respectively, and $\lambda$ is the mixing proportion.

We seek values for the parameter vector $\boldsymbol{\theta} = (\mu_1, \mu_2, \sigma_1, \sigma_2, \lambda)$ such that $\ell(\boldsymbol{\theta})$ becomes as large as possible. This problem cannot be solved analytically but requires numerical optimization.

> **Remark:** We will see that optimization in this example is very fast. This is because the data set is relatively small and we consider a model with two classes only. Therefore, it might not seem relevant to be concerned about initialization here. However, the problem scales: optimization time will rise with more data and more parameters, in which case initialization becomes a greater issue, see for example @Shireman:2017. Additionally, we will see that even this simple optimization problem suffers heavily from local optima, depending on the choice of initial values.

The following function calculates the log-likelihood value $\ell(\boldsymbol{\theta})$ given the parameter vector `theta` and the observation vector `data`:

```{r, define mixture ll}
normal_mixture_llk <- function(mu, sd, lambda, data) {
  sd <- exp(sd)
  lambda <- plogis(lambda)
  sum(log(lambda * dnorm(data, mu[1], sd[1]) + (1 - lambda) * dnorm(data, mu[2], sd[2])))
}
normal_mixture_llk(mu = 1:2, sd = 3:4, lambda = 5, data = faithful$eruptions)
```

> **Remark:** We restrict the standard deviations `sd` to be positive via the exponential transformation and `lambda` to be between 0 and 1 via the logit transformation. In this way we are able to optimize over the value space $\mathbb{R}^5$ and do not require box-constrained optimizers.

### Initialization effect

Does the choice of initial values have an influence for this optimization problem? In the following, we will use the `{ino}` package to optimize the likelihood function, starting from 100 random initial points, and compare the results. 

We start be defining the optimization problem:^[For more details on the `{R6}` syntax that `{ino}` uses, we refer to @Wickham:2019, Chapter 14.]

```{r, initialize Nop}
Nop_mixture <- Nop$new(
  objective = normal_mixture_llk,    # the objective function
  target = c("mu", "sd", "lambda"),  # names of target arguments
  npar = c(2, 2, 1),                 # lengths of target arguments
  data = faithful$eruptions          # values for fixed arguments
)

# TODO: remove
self <- Nop_mixture
private <- self$.__enclos_env__$private
```

The call `Nop$new()` creates a `Nop` object that defines a numerical optimization problem. We saved the object under the name `Nop_mixture`. In the future we can work with this object by calling its methods via `Nop_mixture$<method name>()`. The arguments are 

- `objective`, the so-called objective function to be optimized, 
- `target`, the names of so-called target argument over which `objective` is optimized, 
- `npar`, the length of each of these target arguments, 
- and the additional argument `data`, which is kept fixed during optimization. 

Once the `Nop` object is defined, the objective function can be evaluated at some value `at` for the collapsed target arguments:

```{r, example evaluation}
Nop_mixture$evaluate(at = 1:5) # same value as above
```

Next, we need a numerical optimizer. Here, we choose `stats::nlm()`:

```{r, define optimizer}
nlm <- optimizeR::Optimizer$new("stats::nlm")
Nop_mixture$set_optimizer(nlm)
```

Once an optimizer is defined, optimizing the function is simple: 

1. define initial values with one of the `initialize_*()` methods (see below)
2. call `optimize()`

```{r, example optimization}
Nop_mixture$
  initialize_random(runs = 20)$
  optimize(which_direction = "max")
```

The method `initialize_random(runs = 20)` generates 20 sets of random initial values (by default independently drawn from a standard normal distribution), and `optimize(which_direction = "max")` maximizes the function starting from these values. 

Accessing the optimization results is possible via the `results()` method. By default, it will return a list of 20 entries with all the optimization results. The method also has some filter options, for example accessing only the function value and parameter estimate from the first run is possible via:

```{r, access results}
Nop_mixture$results(which_run = 1, which_element = c("value", "parameter"))
```

The `summary()` method summarizes the results in a data frame:

```{r, summary of results}
Nop_mixture$summary(which_element = c("value", "seconds", "iterations"))
```

For a quick overview of the obtained optima, the `optima()` method provides a frequency table of the function values obtained at optimizer convergence (we can ignore the decimal places via `digits = 0`):

```{r, overview optima}
Nop_mixture$optima(digits = 0)
```

The influence of the initial values on the outcome is evident. Now we may ask ourselves what the two maxima $-276$ and $-421$ imply for our model fit of the Gaussian mixture model to the Geyser data. To compare the parameter vectors that led to these different values, we can use the `$closest()` method: from the obtained optimization runs, it extracts the parameter vector corresponding to an optimum closest (in absolute distance) to a reference `value`:

```{r, closest parameters}
(global <- Nop_mixture$closest(value = -276))
global_run_id <- attr(global, ".run_id")
Nop_mixture$evaluate(as.numeric(global))
(local <- Nop_mixture$closest(value = -421))
local_run_id <- attr(local, ".run_id")
Nop_mixture$evaluate(as.numeric(local))
```

These two parameter vectors are saved here as objects `global` (this presumably is the global maximum) and `local` (a local maximum). Two attributes show the optimization run id and the used optimizer.

To understand the parameter estimates in terms of mean, standard deviation, and mixing proportion (i.e., in the form $\boldsymbol{\theta} = (\mu_1, \mu_2, \sigma_1, \sigma_2, \lambda)$), they need back-transformation to the restricted parameter space $\mathbb{R}^2 \times \mathbb{R}_+^2 \times [0,1]$ (see above):

```{r, transform parameter}
transform <- function(theta) c(theta[1:2], exp(theta[3:4]), plogis(theta[5]))
(global <- transform(global))
(local <- transform(local))
```

The two estimates `global` and `local` for $\boldsymbol{\theta}$ correspond to the following mixture densities:

```{r, estimated-mixtures}
mixture_density <- function (data, mu, sd, lambda) {
  lambda * dnorm(data, mu[1], sd[1]) + (1 - lambda) * dnorm(data, mu[2], sd[2])
}
ggplot(faithful, aes(x = eruptions)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30) + 
  labs(x = "eruption time (min)", colour = "parameter") +
  stat_function(
    fun = function(x) {
      mixture_density(x, mu = global[1:2], sd = global[3:4], lambda = global[5])
    }, aes(color = "global"), linewidth = 1
  ) +
  stat_function(
    fun = function(x) {
      mixture_density(x, mu = local[1:2], sd = local[3:4], lambda = local[5])
    }, aes(color = "local"), linewidth = 1
  )
```

We see that the mixture defined by the `global` parameter fits much better than `local`, which practically estimates only a single class. However, the gradients at both points are close to zero, which explains why the `stats::nlm` optimizer terminates at both points:

```{r, extract gradients}
Nop_mixture$summary(
  which_run = c(global_run_id, local_run_id), which_element = c("value", "gradient")
)
```

### Comparing initialization strategies

How do different initialization strategies perform for this optimization problem? Previously, we saw that the numerical likelihood optimization of the mixture model yield different estimates based on the initial values. This raises the question how to best choose the initial values. The `{ino}` package provides the following initialization methods that can easily be compared: 

| method | purpose |
|--------|---------|
| `initialize_fixed()` | fixed initial values, for example at the origin or educated guesses |
| `initialize_random()` | random initial values drawn from a custom distribution |
| `initialize_grid()` | initial values as grid points (optionally randomly shuffled) |
| `initialize_continue()` | initial values from previous optimization runs with a different optimizer or a simplified optimization problem |
| `initialize_custom()` | initial values based on a custom initialization strategy |

The following methods can be used to modify the initial values:

| method | purpose |
|--------|---------|
| `initialize_promising()` | selects a proportion of promising initial values that correspond to a steep gradient or large/low function value |
| `initialize_transform()` | transforms the initial values |
| `initialize_reset()` | deletes all specified initial values |

We applied the method `initialize_random()` before. In the following, we compare it to `initialize_grid()` in combination with `initialize_promising()`. The other methods are illustrated in the other vignettes.

We make "educated guesses" about starting values that are probably close to the global optimum. Based on the histogram above, the means of the two normal distributions may be somewhere around $2$ and $4$. We will use sets of starting values where the means are lower and larger than $2$ and $4$, respectively. For the variances, we set the starting values close to $1$ (note that we use the log-transformation here since we restrict the standard deviations to be positive by using the exponential function in the likelihood). The starting value for the mixing proportion shall be around $0.5$. We use three grid points in each dimension, which we shuffle via the `jitter = TRUE` argument. This leads to a grid of $3^5 = 243$ starting values:

```{r, grid initial}
Nop_mixture$initialize_grid(
  lower = c(1.5, 3.5, log(0.8), log(0.8), qlogis(0.4)), # lower bounds for the grid
  upper = c(2.5, 4.5, log(1.2), log(1.2), qlogis(0.5)), # upper bounds for the grid
  breaks = c(3, 3, 3, 3, 3),                            # breaks for the grid in each dimension
  jitter = TRUE                                         # random shuffle of the grid points
)
```

From the $243$ grid starting values, we select a $10\%$ proportion of locations at which the objective gradient is steepest:

```{r, steepest gradient}
Nop_mixture$initialize_promising(proportion = 0.1, condition = "gradient_steep")
```


```{r, optimize at steepest gradient initial values}
Nop_mixture$optimize(which_direction = "max", optimization_label = "promising_grid")
```


```{r, overview comparison}
# TODO: allow for grouping
Nop_mixture$optima(which_direction = "max", group_by = ".optimization_label", digits = 0)
```

### Comparing optimizer functions

How do different optimization approaches perform for this optimization problem? So far, we only applied the `stats::nlm` optimizer, which uses a Newton-type algorithm. Now, we will compare its results and optimization time to 

1. `stats::optim`, an alternative R optimizer which (by default) applies the Nelder-Mead algorithm [@Nelder:1965], and 
2. the expectation-maximization algorithm `em_optimizer`, an alternative optimization method for mixture models, which we define in the appendix below. 

```{r, define em already here, include = FALSE}
# TODO: data as ... argument?
em <- function(normal_mixture_llk, theta, epsilon = 1e-08, iterlim = 1000, data) {
  llk <- normal_mixture_llk(theta, data)
  mu <- theta[1:2]
  sd <- exp(theta[3:4])
  lambda <- plogis(theta[5])
  for (i in 1:iterlim) {
    class_1 <- lambda * dnorm(data, mu[1], sd[1])
    class_2 <- (1 - lambda) * dnorm(data, mu[2], sd[2])
    posterior <- class_1 / (class_1 + class_2)
    lambda <- mean(posterior)
    mu[1] <- mean(posterior * data) / lambda
    mu[2] <- (mean(data) - lambda * mu[1]) / (1 - lambda)
    sd[1] <- sqrt(mean(posterior * (data - mu[1])^2) / lambda)
    sd[2] <- sqrt(mean((1 - posterior) * (data - mu[2])^2) / (1 - lambda))
    llk_old <- llk
    theta <- c(mu, log(sd), qlogis(lambda))
    llk <- normal_mixture_llk(theta, data)
    if (is.na(llk)) stop("em failed")
    if (llk - llk_old < epsilon) break
  }
  list("llk" = llk, "estimate" = theta, "iterations" = i)
}
em_optimizer <- optimizeR::Optimizer$new("custom")
em_optimizer$definition(
  algorithm = em,
  arg_objective = "normal_mixture_llk",
  arg_initial = "theta",
  out_value = "llk",
  out_parameter = "estimate",
  direction = "max"
)
```

We add these two optimizers to our `Nop_mixture` object, again via the `{optimizeR}` framework. Here, `em_optimizer` already is an optimizer in the required framework. In the appendix below, we go into detail how this optimizer is implemented.

```{r, set optim and em algorithm}
optim <- optimizeR::Optimizer$new(which = "stats::optim")
Nop_mixture$
  set_optimizer(optim)$
  set_optimizer(em_optimizer)
```

Next, we initialize at $100$ random points and optimize the mixture likelihood with each of the three optimizers from the same points:

```{r, optimizer comparison, eval = FALSE}
Nop_mixture$
  initialize_random(runs = 100)$
  optimize(which_direction = "max", optimization_label = "optimizer_comparison")
```

The `$plot()` method provides a visual comparison of the optimization times and obtained values:

```{r, plot-seconds, eval = FALSE}
Nop_mixture$plot(
  which_element = "seconds",         # plot the optimization times in seconds
  group_by = ".optimizer_label",     # comparison across optimizers
  relative = TRUE,                   # relative differences to the median of the top boxplot
  which_run = "optimizer_comparison" # select only the results from the comparison
  xlim = c(-1, 3)                    # x-axis limits
)
```

```{r, plot-values, eval = FALSE}
Nop_mixture$plot(
  which_element = "value",           # plot the obtained optima values
  group_by = ".optimizer_label"
)
```

We conclude that ...

## Appendix

### The expectation-maximization algorithm

The likelihood function of the mixture model cannot be maximized analytically. But if we knew the class membership of each observation, the optimization problem would collapse to independent maximum likelihood estimation of two Gaussian distributions, which then can be solved analytically. This observation motivates the so-called expectation-maximization (EM) algorithm [@Dempster:1977], which iterates through the following steps:

1. initialize $\boldsymbol{\theta}$ and compute $\ell(\boldsymbol{\theta})$
2. calculate the posterior probabilities for each observation's class membership, conditional on $\boldsymbol{\theta}$
3. calculate the maximum likelihood estimate $\boldsymbol{\bar{\theta}}$ conditional on the posterior probabilities from step 2
4. evaluate $\ell(\boldsymbol{\bar{\theta}})$ and either stop, if the likelihood improvement $\ell(\boldsymbol{\bar{\theta}}) - \ell(\boldsymbol{\theta})$ is smaller than some threshold `epsilon` or some iteration limit `iterlim` is reached, and otherwise return to step 2

The following function implements this algorithm:

```{r, define em algorithm}
# TODO: data as ... argument?
em <- function(normal_mixture_llk, theta, epsilon = 1e-08, iterlim = 1000, data) {
  llk <- normal_mixture_llk(theta, data)
  mu <- theta[1:2]
  sd <- exp(theta[3:4])
  lambda <- plogis(theta[5])
  for (i in 1:iterlim) {
    class_1 <- lambda * dnorm(data, mu[1], sd[1])
    class_2 <- (1 - lambda) * dnorm(data, mu[2], sd[2])
    posterior <- class_1 / (class_1 + class_2)
    lambda <- mean(posterior)
    mu[1] <- mean(posterior * data) / lambda
    mu[2] <- (mean(data) - lambda * mu[1]) / (1 - lambda)
    sd[1] <- sqrt(mean(posterior * (data - mu[1])^2) / lambda)
    sd[2] <- sqrt(mean((1 - posterior) * (data - mu[2])^2) / (1 - lambda))
    llk_old <- llk
    theta <- c(mu, log(sd), qlogis(lambda))
    llk <- normal_mixture_llk(theta, data)
    if (is.na(llk)) stop("em failed")
    if (llk - llk_old < epsilon) break
  }
  list("llk" = llk, "estimate" = theta, "iterations" = i)
}
```

### Defining optimizers via the `{optimizeR}` framework

```{r, define em optimizeR}
em_optimizer <- optimizeR::Optimizer$new("custom")
em_optimizer$definition(
  algorithm = em,
  arg_objective = "normal_mixture_llk",
  arg_initial = "theta",
  out_value = "llk",
  out_parameter = "estimate",
  direction = "max"
)
```

## References
