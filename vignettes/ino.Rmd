---
title: "Introduction"
description: >
  Learn how to get started with the {ino} package.
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
link-citations: true
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse  = TRUE,
  comment   = "#>",
  fig.align = "center",
  fig.path  = "figures/ino-",
  fig.dim   = c(8, 6), 
  out.width = "75%"
)
```

## Motivation

Optimization refers to making something as effective, efficient, or functional as possible. This goal is important in many fields, for example optimal portfolio composition in finance, minimizing air resistance in engineering, or likelihood maximization for model fitting in statistics. In any case, assuming a function can be defined that maps inputs to outputs, the goal of optimization is to find inputs such that the output is in some sense optimal.

In some cases, the function is simple enough that finding the optimal value is possible analytically: for example, if $f:\mathbb{R} \to \mathbb{R},\ f(x) = -x^2$, we find that the first derivative $f'(x) = -2x$ vanishes at $x = 0$, and since $f$ is strictly concave, this $x$ must be the unique point where $f(x)$ is maximal. But often, the optimization problem at hand cannot be solved analytically, for example when explicit formulas for derivatives are unknown or do not exist. In these cases, optimization is still possible via a concept called numerical optimization.

Numerical optimization refers to any algorithm that iteratively explores the parameter space, guaranteeing to improve the function output over each iteration, and eventually converging to a point where no more improvements can be made [@Bonnans:2006]. Several such algorithms are implemented in R.^[The CRAN Task View: Optimization and Mathematical Programming [@Schwendinger:2023] provides a comprehensive list of packages for solving optimization problems numerically.] One thing that all of these algorithms have in common is that initial parameter values must be specified, i.e., the point from where the procedure is started. But choosing initial values is non-trivial and can be a challenging task.

The reason why initialization is so challenging is that starting values can have a large influence on the optimization time and optimization result [@Nocedal:2006]. In general, starting in areas of function saturation increases computation time, starting in areas of non-concavity leads to convergence problems or convergence to local rather than global optima. The increase in computation time is a problem for complex functions where each iteration of the optimizer takes a significant evaluation time, while the concern of convergence issues to the global optimum is problematic in any case. So two questions immediately arise:

1. Is my optimization problem affected by initialization?

2. If so, what are good initial values in the sense that optimization is fast and leads to the global optimum?

## Package functionality

We introduce the `{ino}` R package (acronym for initialization of numerical optimization), which helps in finding answers to the questions above. In particular, the package provides tools for three tasks:

1. exploring the effect of the initial values on the optimization

2. comparing different initialization strategies

3. comparing different numerical optimizers

The package follows an object-oriented approach^[We make use of the object-oriented framework of the `{R6}` package [@Chang:2021].], treating numerical optimization problems as objects. The object is defined by a real-valued function, the argument over which the function is to be optimized, and one or more optimization algorithms. The object provides methods for choosing initial values, carrying out numerical optimization, and evaluating the optimization results.

The main benefits of using the package are as follows:

- Any optimizer implemented in R can be applied for minimization and maximization tasks.^[To achieve consistency across implementations of optimization algorithms in R, we make use of the framework provided by the `{optimizeR}` package [@Oelschlaeger:2023].]

- Generally applicable initialization strategies are implemented, and users can extend custom strategies.

- Methods allow for easy comparisons across optimizers and initialization strategies, and to finally extract the best optimization result.

- The package supports parallel computation and progress updates.

- We put effort into a user-friendly workflow: every input is documented in detail, every user input is validated, and users only interact with a single object. 

## Example workflow

To illustrate the package workflow, we solve a likelihood optimization problem. In a first step, `{ino}` can be obtained from [CRAN](https://CRAN.R-project.org) via:

```{r, load ino}
# install.packages("ino")
# library("ino")
devtools::load_all() # TODO: remove
```

### Gaussian mixture model

The function to be optimized in this example is a likelihood function, which computes the probability of observing given data under a model assumption. The parameters that maximize the likelihood function are then identified as the model estimates. This approach is called maximum likelihood estimation and is very popular in statistics for fitting a model to empirical data.

We consider eruption times of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The data histogram hints at two clusters with short and long eruption times, respectively:

```{r, faithful plot, warning = FALSE}
library("ggplot2")
ggplot(faithful, aes(x = eruptions)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30) + 
  xlab("eruption time (min)") 
```

For both clusters, we assume a normal distribution, such that we consider a mixture of two Gaussian densities for modeling the overall eruption times. The log-likelihood function^[Optimizing the log-likelihood function is equivalent to optimizing the likelihood function, because likelihoods are positive and the logarithm is a monotone transformation. But optimizing the log-likelihood function has numerical advantages, in particular avoiding numerical underflow when the likelihood becomes small.] is defined by

\begin{equation}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \log\Big( \lambda \phi_{\mu_1, \sigma_1^2}(x_i) + (1-\lambda)\phi_{\mu_2,\sigma_2^2} (x_i) \Big),
\end{equation}

where the sum goes over all observations $1, \dots, n = 272$, $\phi_{\mu_1, \sigma_1^2}$ and $\phi_{\mu_2, \sigma_2^2}$ denote the normal density for the first and second cluster, respectively, and $\lambda$ is the mixing proportion.

We seek values for the parameter vector $\boldsymbol{\theta} = (\mu_1, \mu_2, \sigma_1, \sigma_2, \lambda)$ such that $\ell(\boldsymbol{\theta})$ becomes as large as possible. This problem cannot be solved analytically but requires numerical optimization.

> **Remark:** We will see that optimization in this example is very fast. This is because the data set is relatively small and we consider a model with two classes only. Therefore, it might not seem relevant to be concerned about initialization here. However, the problem scales: optimization time will rise with more data and more parameters, in which case initialization becomes a greater issue, see for example @Shireman:2017. Additionally, we will see that even this simple optimization problem suffers heavily from local optima, depending on the choice of initial values.

The following function calculates the log-likelihood value $\ell(\boldsymbol{\theta})$ given the parameter vector `theta` and the observation vector `data`:

```{r, define mixture ll}
normal_mixture_llk <- function(mu, sd, lambda, data) {
  sd <- exp(sd)
  lambda <- plogis(lambda)
  sum(log(lambda * dnorm(data, mu[1], sd[1]) + (1 - lambda) * dnorm(data, mu[2], sd[2])))
}
normal_mixture_llk(mu = 1:2, sd = 3:4, lambda = 5, data = faithful$eruptions)
```

> **Remark:** We restrict the standard deviations `sd` to be positive via the exponential transformation and `lambda` to be between 0 and 1 via the logit transformation. In this way we are able to optimize over the value space $\mathbb{R}^5$ and do not require box-constrained optimizers.

### Initialization effect

Does the choice of initial values have an influence for this optimization problem? In the following, we will use the `{ino}` package to optimize the likelihood function, starting from 100 random initial points, and compare the results. 

We start be defining the optimization problem:^[For more details on the `{R6}` syntax that `{ino}` uses, we refer to @Wickham:2019, Chapter 14.]

```{r, initialize Nop}
Nop_mixture <- Nop$new(
  objective = normal_mixture_llk,    # the objective function
  target = c("mu", "sd", "lambda"),  # names of target arguments
  npar = c(2, 2, 1),                 # lengths of target arguments
  data = faithful$eruptions          # values for fixed arguments
)
```

The call `Nop$new()` creates a `Nop` object that defines a numerical optimization problem. We saved the object under the name `Nop_mixture`. In the future we can work with this object by calling its methods via `Nop_mixture$<method name>()`. The arguments are 

- `objective`, the so-called objective function to be optimized, 
- `target`, the names of so-called target argument over which `objective` is optimized, 
- `npar`, the length of each of these target arguments, 
- and the additional argument `data`, which is kept fixed during optimization. 

Once the `Nop` object is defined, the objective function can be evaluated at some value `at` for the collapsed target arguments:

```{r, example evaluation}
Nop_mixture$evaluate(at = 1:5) # same value as above
```

Next, we need a numerical optimizer. Here, we choose `stats::nlm()`:

```{r, define optimizer}
nlm <- optimizeR::Optimizer$new("stats::nlm")
Nop_mixture$set_optimizer(nlm)
```

Once an optimizer is defined, optimizing the function is simple: 

1. define initial values with one of the `initialize_*()` methods (see below)
2. call `optimize()`

```{r, example optimization}
Nop_mixture$
  initialize_random(runs = 100)$
  optimize(which_direction = "max")
```

The method `initialize_random(runs = 100)` generates 100 sets of random initial values (by default independently drawn from a standard normal distribution), and `optimize(which_direction = "max")` maximizes the function starting from these values. 

# TODO

Finally, accessing the optimization results is possible via the `results()` method. By default, it will return a list of 100 entries with all the optimization results. The method also has some filter options, for example accessing only the function value and parameter estimate from the 42th run is possible via:

```{r, access results, eval = FALSE}
Nop_mixture$results(which_run = 42, which_element = c("value", "parameter"))
```

The `summary` method summarizes the results in a data.frame.

```{r, summary of results, eval = FALSE}
Nop_mixture$optima()
```

For a quick overview, the `optima()` method provides a frequency table of the function values obtained at optimizer convergence for the random initial values: 

```{r, overview optima, eval = FALSE}
Nop_mixture$optima()
```

The influence of the initial values on the outcome is evident: we find that two values are obtained quite often. 

But what is the implication on the model fit? Use method `best()` to get best.

### Comparing initialization strategies

How do different initialization strategies perform for this optimization problem? The `{ino}` package implements the following initialization strategies:

- fixed (for example at zero, or educated guesses)
- random (different distributions)
- grid (random shuffle)
- transfer (instead of "continue"!, transfer from other optimization, subset, standardize)
- promising (use current initial values, only those with high gradient / large function value)

add custom strategy?

### Comparing optimizer functions

How do different optimization approaches perform for this optimization problem? So far, we only applied the `stats::nlm` optimizer. Now, we will compare its results and optimization time to the `stats::optim` optimizer and the expectation-maximization algorithm. 

Clear results, start from same values.

However, if we knew the class membership of each observation, the optimization problem would collapse to independent maximum likelihood estimation of two Gaussian distributions, which then can be solved analytically. This observation motivates the so-called expectation-maximization (EM) algorithm [@Dempster:1977], which iterates through the following steps:

1. Initialize $\boldsymbol{\theta}$ and compute $\ell(\boldsymbol{\theta})$.
2. Calculate the posterior probabilities for each observation's class membership, conditional on $\boldsymbol{\theta}$.
3. Calculate the maximum likelihood estimate $\boldsymbol{\bar{\theta}}$ conditional on the posterior probabilities from step 2.
4. Evaluate $\ell(\boldsymbol{\bar{\theta}})$. Now stop if the likelihood improvement $\ell(\boldsymbol{\bar{\theta}}) - \ell(\boldsymbol{\theta})$ is smaller than some threshold `epsilon` or some iteration limit `iterlim` is reached. Otherwise, go back to step 2.

The following function implements this algorithm:

```{r, define em algorithm}
em <- function(normal_mixture_llk, theta, epsilon = 1e-08, iterlim = 1000, data) {
  llk <- normal_mixture_llk(theta, data)
  mu <- theta[1:2]
  sd <- exp(theta[3:4])
  lambda <- plogis(theta[5])
  for (i in 1:iterlim) {
    class_1 <- lambda * dnorm(data, mu[1], sd[1])
    class_2 <- (1 - lambda) * dnorm(data, mu[2], sd[2])
    posterior <- class_1 / (class_1 + class_2)
    lambda <- mean(posterior)
    mu[1] <- mean(posterior * data) / lambda
    mu[2] <- (mean(data) - lambda * mu[1]) / (1 - lambda)
    sd[1] <- sqrt(mean(posterior * (data - mu[1])^2) / lambda)
    sd[2] <- sqrt(mean((1 - posterior) * (data - mu[2])^2) / (1 - lambda))
    llk_old <- llk
    theta <- c(mu, log(sd), qlogis(lambda))
    llk <- normal_mixture_llk(theta, data)
    if (is.na(llk)) stop("em failed")
    if (llk - llk_old < epsilon) break
  }
  list("llk" = llk, "estimate" = theta, "iterations" = i)
}
```


```{r, set em algorithm, eval = FALSE}
em_optimizer <- optimizeR::define_optimizer(
  .optimizer = em, .objective = "normal_mixture_llk",
  .initial = "theta", .value = "neg_llk", .parameter = "estimate",
  .direction = "max"
)
Nop_mixture$
  set_optimizer(em_optimizer, optimizer_label = "em")
```

## References
